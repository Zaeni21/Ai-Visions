<!DOCTYPE html><html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>AI Vision HUD Super</title>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/vision_bundle.js"></script>
  <style>
    body, html {
      margin: 0;
      padding: 0;
      background: black;
      font-family: 'Orbitron', sans-serif;
      overflow: hidden;
    }
    video, canvas {
      position: fixed;
      top: 0; left: 0;
      width: 100vw;
      height: 100vh;
      object-fit: cover;
      z-index: 1;
    }
    #canvas {
      z-index: 2;
      pointer-events: none;
    }
    .hudText, .controls {
      position: absolute;
      z-index: 4;
      top: 10px;
      left: 10px;
      background: rgba(0, 255, 0, 0.1);
      padding: 10px;
      border: 1px solid #0f0;
      border-radius: 10px;
      text-shadow: 0 0 5px #0f0;
      color: #0f0;
    }
    .controls {
      top: auto;
      bottom: 10px;
      display: flex;
      gap: 10px;
      left: 10px;
      flex-wrap: wrap;
    }
    button {
      background: #0f0;
      color: black;
      font-family: 'Orbitron';
      border: none;
      padding: 10px;
      border-radius: 5px;
      cursor: pointer;
    }
    .glitch, .grid {
      position: fixed;
      width: 100vw;
      height: 100vh;
      z-index: 3;
      pointer-events: none;
    }
    .glitch {
      background: repeating-linear-gradient(0deg, rgba(0,255,0,0.02), rgba(0,255,0,0.02) 1px, transparent 1px, transparent 2px);
      mix-blend-mode: screen;
    }
    .grid {
      background: repeating-linear-gradient(90deg, rgba(0,255,255,0.05) 0px, rgba(0,255,255,0.05) 1px, transparent 1px, transparent 40px),
                  repeating-linear-gradient(0deg, rgba(0,255,255,0.05) 0px, rgba(0,255,255,0.05) 1px, transparent 1px, transparent 40px);
    }
  </style>
  <link href="https://fonts.googleapis.com/css2?family=Orbitron:wght@500&display=swap" rel="stylesheet">
</head>
<body>
  <video id="video" autoplay muted playsinline></video>
  <canvas id="canvas"></canvas>
  <div class="glitch"></div>
  <div class="grid"></div>
  <div class="hudText" id="hudText">AI: Initializing...</div>
  <div class="controls">
    <button onclick="toggleNightVision()">Night Vision</button>
    <button onclick="toggleThermal()">Thermal</button>
    <button onclick="togglePredator()">Predator</button>
    <button onclick="toggleGrid()">Grid</button>
    <button onclick="captureScreenshot()">Screenshot</button>
    <button onclick="startRecording()">Start Rec</button>
    <button onclick="stopRecording()">Stop Rec</button>
  </div>  <script>
    const video = document.getElementById('video');
    const canvas = document.getElementById('canvas');
    const ctx = canvas.getContext('2d');
    const hudText = document.getElementById('hudText');
    const grid = document.querySelector('.grid');
    canvas.width = window.innerWidth;
    canvas.height = window.innerHeight;

    let nightVision = false, thermal = false, predator = false, showGrid = true;

    function toggleNightVision() {
      nightVision = !nightVision;
      document.body.style.background = nightVision ? '#030' : 'black';
    }
    function toggleThermal() { thermal = !thermal; }
    function togglePredator() { predator = !predator; }
    function toggleGrid() { grid.style.display = (showGrid = !showGrid) ? 'block' : 'none'; }

    let recorder, recordedChunks = [];
    function startRecording() {
      const stream = canvas.captureStream();
      recorder = new MediaRecorder(stream);
      recorder.ondataavailable = e => recordedChunks.push(e.data);
      recorder.onstop = () => {
        const blob = new Blob(recordedChunks, { type: 'video/webm' });
        const url = URL.createObjectURL(blob);
        const a = document.createElement('a');
        a.href = url;
        a.download = 'recorded-ai.webm';
        a.click();
        recordedChunks = [];
      };
      recorder.start();
    }
    function stopRecording() { if (recorder) recorder.stop(); }
    function captureScreenshot() {
      const link = document.createElement('a');
      link.download = 'screenshot-ai.png';
      link.href = canvas.toDataURL();
      link.click();
    }

    async function startAI() {
      const vision = await FilesetResolver.forVisionTasks(
        'https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm'
      );
      const objectDetector = await ObjectDetector.createFromOptions(vision, {
        baseOptions: { modelAssetPath: 'https://storage.googleapis.com/mediapipe-models/object_detector/lite_model/float16.tflite' },
        scoreThreshold: 0.5,
        runningMode: 'VIDEO'
      });

      function render() {
        ctx.clearRect(0, 0, canvas.width, canvas.height);
        if (thermal) ctx.fillStyle = 'rgba(255,0,0,0.1)', ctx.fillRect(0, 0, canvas.width, canvas.height);
        if (predator) ctx.fillStyle = 'rgba(255,0,255,0.1)', ctx.fillRect(0, 0, canvas.width, canvas.height);
        const now = performance.now();
        objectDetector.detectForVideo(video, now, results => {
          results.detections.forEach(det => {
            const box = det.boundingBox;
            ctx.strokeStyle = predator ? 'magenta' : (thermal ? 'red' : '#0f0');
            ctx.lineWidth = 2;
            ctx.strokeRect(box.originX, box.originY, box.width, box.height);
            ctx.fillStyle = ctx.strokeStyle;
            ctx.fillText(det.categories[0].categoryName, box.originX, box.originY - 5);
          });
        });
        requestAnimationFrame(render);
      }
      render();
    }

    navigator.mediaDevices.getUserMedia({ video: { facingMode: 'environment' } })
      .then(stream => {
        video.srcObject = stream;
        video.onloadeddata = () => startAI();
      });
  </script></body>
</html>
